{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Voice Agent for Data Capture\n",
        "\n",
        "Technical stack with LangGraph:\n",
        "\n",
        "Orchestrator Agent is GPT-4o\n",
        "\n",
        "Data Aggregator functionality to read, retrieve, and write to existing database with connection string.\n",
        "\n",
        "Tools:\n",
        "- Tool to retrieve schema from database (use case: LLM needs to locate where in the schema to retrieve or write data to).\n",
        "- Tool to retrieve data samples from database (use case: LLM needs to few shot itself/another LLM with data samples for pre-labeling).\n",
        "- Tool to write data to the database (use case: after locating the part of the schema to add data to and getting the information from the user, the LLM needs to write data to that part of the database).\n",
        "- Tool to create workflows on the fly (this can be done internally through the LLM. However, there needs to be other tools to collect data from the user, such as a tool to open a camera modal or tool to collect a voice recording).\n",
        "- Tool to prelabel data (use case: using the tool to retrieve data to few shot an LLM to pre-label data. This can either be the orchestrator agent or another smaller or just separate LLM. )\n",
        "- Tool to retrieve all tools available to the LLM.\n",
        "\n",
        "Example workflow:\n",
        "\n",
        "After connecting to a SQL relational database through a connection string, the LLM reads the schemas and database and 'understands' it.\n",
        "\n",
        "Then, the user asks the LLM to \"please record a new ‘bag-dump’ of fine-ware pottery sherds. \"\n",
        "\n",
        "The LLM takes in the input, reasons through it, and develops a plan/workflow (just like the o1 reasoning models do). It does so by first locating the part of the schema to add data to by calling on the tool to retrieve schema from the user's existing database. It determines that these sherds need to be 'fine ware' entry samples, and to enter it into the samples, it needs to gather the type, categorization, and weight of the sherds. From the few-shot prompt, it develops a workflow to first get the bag-dump image from the user by prompting the user to take an image, then call prelabeling tools to do object detecting and classifications on them, and have the user do HITL verification of all prelabeling and edit if necessary before commiting to the database.\n",
        "\n",
        "After it reasons through and develops that plan, it first explains what its going to do to the user through voice and then asks the user if that sounds good and proceed. If the user responds with anything else, it will make the necessary adjustments. Afterward, it executes it by calling the tools in that order and prompting the user step by step do collect the data while also listening to the user through the whole process for questions or potential plan or workflow adjustments. After the verification, it will call on the writing tool to commit it to the database. It also logs it into a separate commitments file to keep track of all commitments and generate paradata and metadata associated with it to enable for instant rollback.\n"
      ],
      "metadata": {
        "id": "ov_LEIUK_4_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refined Example Workflow\n",
        "\n",
        "After connecting and learning database.\n",
        "\n",
        "1) User holds down and speaks: \"Can you please record a new ‘bag-dump’ of fine-ware pottery sherds. \"\n",
        "\n",
        "2) Agent (reasoning loop):\n",
        "- Calls introspectSchema to return the cached/hard-coded schema table.\n",
        "- LLM reasons and locates part of schema to write to + information it needs to collect.\n",
        "- Agent builds PLAN JSON via few-shotted PLAN templates.\n",
        "\n",
        "  \"steps\": [\n",
        "    { \"ask\": \"photo\", \"prompt\": \"Please photograph the bag-dump.\" },\n",
        "\n",
        "    { \"tool\": \"VisionLabel\", \"args\": { \"model\": \"yolo-nano\" } },\n",
        "\n",
        "    { \"ask\": \"voice\",\n",
        "      \"prompt\": \"I found {{count}} sherds. I’ll show each close-up—confirm rim/base and weight or say 'skip'.\" },\n",
        "\n",
        "    { \"tool\": \"WriteRows\" }\n",
        "  ]\n",
        "}\n",
        "\n",
        "- Agent reads verbal summary to user and asks for confirmation before execution.\n",
        "\n",
        "3) The agent continues to technically stay in the loop while calling the various tools.\n",
        "- On the front end, the user is prompted to take an image by a modal.\n",
        "- After the image, the LLM takes the user through each identified and prelabeled sherd in the photo by zooming in and asking for confirmation. It listens for the user's questions and potential changes to the prelabeling.\n",
        "- Afterward, the LLM commits the changes to the database. Agent calls WriteRows, returns commit IDs. VersionedWriter appends {row_id, diff, thought} to audit_log.\n",
        "\n",
        "For initial prototype + demo:\n",
        "1. Stream mic-to-cloud via Whisper and GPT-4o\n",
        "2. Hard code schema returned in the introspectSchema function for a demo SQL relational database.\n",
        "\n",
        "Main points of demonstration:\n",
        "1. A conversational voice agent finds the correct information to record, develops a reasonable human-in-the-loop workflow for it, and is able to write to the database.\n",
        "2. Audit log exists and can be viewed. Instant rollback is possible.\n",
        "3. Powerful, accurate, and versatile prelabeling to save time."
      ],
      "metadata": {
        "id": "dSkr5oE1Ho3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For database connection via connection string:\n",
        "\n",
        "We can use Supabase as a thin proxy/wrapper around another Postgres (MySQL, SQL Server, etc).\n",
        "\n",
        "To do so, we will create an empty Supabase project, install the postgres_fdw or dblink extension to bring the external tables and data from the external PostGres in, and control the REST/GraphQL as if the tables were actually local.\n"
      ],
      "metadata": {
        "id": "9Q5gb50XwCRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember to add the supervisor in the FastAPI --> every user 'utterance' gets passed through the small LLM to be either classified as pass/answer/replan.**"
      ],
      "metadata": {
        "id": "wWA8-ENQfQsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Rmm6wQ_uZc"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai langgraph langchain-core openai replicate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "vrfTwoTTDega"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "2ZRau-2JDeOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools"
      ],
      "metadata": {
        "id": "duJAr8XCOamd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def introspect_schema() -> Dict[str, Any]:\n",
        "  \"\"\"Return hard coded JSON schema file --> swap this for a live Supabase GraphQL wrapper introspection query. \"\"\"\n",
        "  return {\n",
        "      \"table\": \"samples\",\n",
        "      \"columns\": {\n",
        "          \"id\": \"uuid\",\n",
        "          \"class\": \"text\",\n",
        "          \"weight\": \"float\",\n",
        "          \"image_url\": \"text\",\n",
        "          \"created_at\": \"timestamptz\"\n",
        "      },\n",
        "      \"primary_key\": \"id\"\n",
        "  }\n",
        "\n",
        "async def sample_rows(n: int = 5) -> List[Dict[str, Any]]:\n",
        "  \"\"\"Return data samples (for few-shotting)\"\"\"\n",
        "  url = f\"{SUPABASE_URL}/rest/v1/samples?limit={n}&select=*\"\n",
        "  r   = await http.get(url, headers=HEADERS_JSON)\n",
        "  r.raise_for_status()\n",
        "  return r.json()\n",
        "\n",
        "class Detection(TypedDict):\n",
        "  bbox: List[float]\n",
        "  label: str\n",
        "  conf: float\n",
        "\n",
        "async def vision_label(image_url: str,\n",
        "                       n_fewshot: int = 6,\n",
        "                       yolo_conf: float = 0.25) -> List[Detection]:\n",
        "  # 1) YOLO detect\n",
        "  yolo_payload = {\n",
        "      \"version\": YOLO_VERSION,\n",
        "      \"input\": { \"image\": image_url, \"conf\": yolo_conf, \"iou\": 0.5 }\n",
        "  }\n",
        "  r = await http.post(\"https://api.replicate.com/v1/predictions\",\n",
        "                      headers={\"Authorization\": f\"Token {REPLICATE_KEY}\",\n",
        "                                \"Content-Type\": \"application/json\"},\n",
        "                      json=yolo_payload)\n",
        "  r.raise_for_status()\n",
        "  pred = r.json()\n",
        "  while pred[\"status\"] not in (\"succeeded\", \"failed\"):\n",
        "    await asyncio.sleep(0.8)\n",
        "    poll = await http.get(pred[\"urls\"][\"get\"],\n",
        "                          headers={\"Authorization\": f\"Token {REPLICATE_KEY}\"})\n",
        "    pred = poll.json()\n",
        "  if pred[\"status\"] == \"failed\":\n",
        "    raise RuntimeError(pred[\"error\"])\n",
        "\n",
        "  # YOLO output: list of {\"x1\":..,\"y1\":..,\"x2\":..,\"y2\":..,\"confidence\":..}\n",
        "  raw_boxes = pred[\"output\"]\n",
        "\n",
        "  # 2) few‑shot examples from DB\n",
        "  examples = await sample_rows(n_fewshot)\n",
        "  classes  = list({row[\"class\"] for row in examples})\n",
        "\n",
        "  # Build few‑shot messages\n",
        "  shots = []\n",
        "  for ex in examples[:4]:\n",
        "      shots.extend([\n",
        "          {\"role\":\"user\",\n",
        "            \"content\":f\"bbox:[0.1,0.2,0.3,0.4] base_conf:0.88\"},\n",
        "          {\"role\":\"assistant\",\"content\":ex[\"class\"]}\n",
        "      ])\n",
        "\n",
        "  # 3) classify each detection with GPT‑4o\n",
        "  out: List[Detection] = []\n",
        "  for box in raw_boxes:\n",
        "    prompt = [\n",
        "        {\"role\":\"system\",\n",
        "          \"content\": \"You are a pottery‑sherd classifier. \"\n",
        "                    \"Respond with exactly one label from this list:\\n\"\n",
        "                    + \", \".join(classes)},\n",
        "        *shots,\n",
        "        {\"role\":\"user\",\n",
        "          \"content\": (f\"bbox:[{box['x1']:.2f},{box['y1']:.2f},\"\n",
        "                      f\"{box['x2']:.2f},{box['y2']:.2f}] \"\n",
        "                      f\"base_conf:{box['confidence']:.2f}\") }\n",
        "    ]\n",
        "    comp = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    temperature=0,\n",
        "                    messages=prompt)\n",
        "    label = comp.choices[0].message.content.strip()\n",
        "    out.append({\"bbox\":[box[\"x1\"],box[\"y1\"],box[\"x2\"],box[\"y2\"]],\n",
        "                \"label\":label,\n",
        "                \"conf\":box[\"confidence\"]})\n",
        "  return out\n",
        "\n",
        "async def write_rows(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "  \"\"\"Commit multiple samples/rows to database + writes audit entry\"\"\".\n",
        "  commit_id = str(uuid.uuid4())\n",
        "  ts        = int(time.time())\n",
        "\n",
        "  # 4.1 bulk insert the samples\n",
        "  ins = await http.post(f\"{SUPABASE_URL}/rest/v1/samples\",\n",
        "                        headers=HEADERS_JSON | {\"Prefer\": \"return=representation\"},\n",
        "                        json=[{**row, \"id\": str(uuid.uuid4())} for row in rows])\n",
        "  ins.raise_for_status()\n",
        "  inserted = ins.json()\n",
        "\n",
        "  # 4.2 write an audit record\n",
        "  audit_body = {\n",
        "      \"id\": commit_id,\n",
        "      \"timestamp\": ts,\n",
        "      \"rows\": [r[\"id\"] for r in inserted],\n",
        "      \"diff\": rows\n",
        "  }\n",
        "  await http.post(f\"{SUPABASE_URL}/rest/v1/audit_log\",\n",
        "                  headers=HEADERS_JSON,\n",
        "                  json=audit_body)\n",
        "\n",
        "  return {\"commit_id\": commit_id, \"row_ids\": [r[\"id\"] for r in inserted]}\n",
        "\n",
        "#Edit frontend to include these functionalities\n",
        "\n",
        "async def ask_photo() -> Dict[str, str]:\n",
        "  \"\"\"Prompts user to take a picture\"\"\"\n",
        "  return {\"prompt\": \"Please photograph the bag.\"}\n",
        "\n",
        "async def user_audio(prompt: str) -> Dict[str, str]:\n",
        "  \"\"\"Prompts user to record audio\"\"\"\n",
        "  return {\"prompt\": prompt}\n",
        "\n",
        "async def user_info(prompt: str) -> Dict[str, str]:\n",
        "  \"\"\"Prompts user to say/type in some information\"\"\"\n",
        "  return {\"prompt\": prompt}\n",
        "\n",
        "\n",
        "tools = [\n",
        "    ToolNode(\"IntrospectSchema\", introspect_schema),\n",
        "    ToolNode(\"SampleRows\", sample_rows),\n",
        "    ToolNode(\"VisionDetectLabel\", vision_label),\n",
        "    ToolNode(\"WriteRows\", write_rows),\n",
        "    ToolNode(\"ask_photo\", ask_photo),\n",
        "    ToolNode(\"UserAudio\", user_audio),\n",
        "    ToolNode(\"UserInfo\", user_info)\n",
        "]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "VYQMNo_POfJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nodes"
      ],
      "metadata": {
        "id": "LwSdUZLpPQKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts user message to structured intent; ask follow-up question if unclear\n",
        "def intent_node(state):\n",
        "  user_msg = state[\"messages\"][-1][\"content\"]\n",
        "  sys_prompt = \"Extract user intent as JSON {task_type,domain}\"\n",
        "  intent = llm_with_tools.invoke(sys_prompt + user_msg).json()\n",
        "  return {\"intent\": intent, \"thinking\": \"parsed intent\"}\n",
        "\n",
        "#Takes in structured intent and cached schema and creates a PLAN JSON workflow w/ self-explanation/chain-of-thought\n",
        "def planner_node(state):\n",
        "  schema = state.get(\"schema\") or introspect_schema(None) #retrieves schmea\n",
        "  plan_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\",\"You are a planner, develop workflow according to user query or intent and the database schema. Here's a template to follow (). \"), (\"user\", \"{intent}\"), (\"system\", \"{schema}\")\n",
        "  ])\n",
        "  plan = llm_with_tools.invoke(\n",
        "      plan_prompt.format(intent=state[\"intent\"], schema=schema)\n",
        "  ).json()\n",
        "  return {\"plan\": plan, \"thinking\": \"built plan\"}\n",
        "\n",
        "#Walks through the JSON PLAN steps (add more for future demos)\n",
        "def runner_node(state):\n",
        "  for step in plan[\"steps\"]:\n",
        "    if step.get(\"ask\") == \"photo\":\n",
        "      yield {\"tool_events\":[\"ask_photo\"]}\n",
        "    elif step.get(\"tool\") == \"VisionDetectLabel\":\n",
        "      detections = vision_label(image_url)\n",
        "      yield {\"tool_events\":[{\"VisionDetectLabel\": detections}],\n",
        "            \"rows_pending\": convert(detections)}\n",
        "  return {}\n",
        "\n",
        "#Commits to database\n",
        "def writer_node(state):\n",
        "  commit = write_rows(state[\"rows_pending\"])\n",
        "  return {\"commit_ids\": commit}\n",
        "\n",
        "sg = StateGraph()\n",
        "sg.add_node(\"Intent\", intent_node)\n",
        "sg.add_node(\"Planner\", planner_node)\n",
        "sg.add_node(\"Runner\", runner_node)\n",
        "sg.add_node(\"Writer\", writer_node)\n",
        "\n",
        "sg.add_edge(START, \"Intent\")\n",
        "sg.add_edge(\"Intent\", \"Planner\")\n",
        "sg.add_edge(\"Planner\", \"Runner\")\n",
        "sg.add_edge(\"Runner\", \"Writer\")\n",
        "sg.add_edge(\"Writer\", END)\n",
        "\n",
        "agent = sg.compile()"
      ],
      "metadata": {
        "id": "ZBbTshSRQjps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}